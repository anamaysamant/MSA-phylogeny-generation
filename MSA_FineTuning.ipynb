{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff10eb32-a31e-44fc-90ee-f89f98740b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1ea35724700>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import esm\n",
    "import torch\n",
    "from time import time\n",
    "\n",
    "from typing import List, Tuple, Optional, Dict, NamedTuple, Union, Callable\n",
    "import itertools\n",
    "import os\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from Bio import SeqIO, Phylo\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import squareform, pdist, cdist\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8b5db2-6471-4ac4-9b76-21b9dfedaecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "deletekeys = dict.fromkeys(string.ascii_lowercase)\n",
    "deletekeys[\".\"] = None\n",
    "deletekeys[\"*\"] = None\n",
    "translation = str.maketrans(deletekeys)\n",
    "\n",
    "def read_sequence(filename: str) -> Tuple[str, str]:\n",
    "    \"\"\" Reads the first (reference) sequences from a fasta or MSA file.\"\"\"\n",
    "    record = next(SeqIO.parse(filename, \"fasta\"))\n",
    "    return record.description, str(record.seq)\n",
    "\n",
    "def remove_insertions(sequence: str) -> str:\n",
    "    \"\"\" Removes any insertions into the sequence. Needed to load aligned sequences in an MSA. \"\"\"\n",
    "    return sequence.translate(translation)\n",
    "\n",
    "def read_msa(filename: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\" Reads the sequences from an MSA file, automatically removes insertions.\"\"\"\n",
    "    return [(record.description, remove_insertions(str(record.seq))) for record in SeqIO.parse(filename, \"fasta\")]\n",
    "\n",
    "def greedy_select(msa: List[Tuple[str, str]], num_seqs: int, mode: str = \"max\") -> List[Tuple[str, str]]:\n",
    "    assert mode in (\"max\", \"min\")\n",
    "    if len(msa) <= num_seqs:\n",
    "        return msa\n",
    "    \n",
    "    array = np.array([list(seq) for _, seq in msa], dtype=np.bytes_).view(np.uint8)\n",
    "\n",
    "    optfunc = np.argmax if mode == \"max\" else np.argmin\n",
    "    all_indices = np.arange(len(msa))\n",
    "    indices = [0]\n",
    "    pairwise_distances = np.zeros((0, len(msa)))\n",
    "    for _ in range(num_seqs - 1):\n",
    "        dist = cdist(array[indices[-1:]], array, \"hamming\")\n",
    "        pairwise_distances = np.concatenate([pairwise_distances, dist])\n",
    "        shifted_distance = np.delete(pairwise_distances, indices, axis=1).mean(0)\n",
    "        shifted_index = optfunc(shifted_distance)\n",
    "        index = np.delete(all_indices, indices)[shifted_index]\n",
    "        indices.append(index)\n",
    "    indices = sorted(indices)\n",
    "    return [msa[idx] for idx in indices]\n",
    "\n",
    "def Seq_tuples_to_fasta(sequences, file_path, export_type = \"fasta\"):\n",
    "    MSA_SeqRecords = [SeqRecord(Seq(record[1]), id = record[0], name= record[0], description= record[0]) for record in sequences]\n",
    "    with open(f\"{file_path}\", \"w\") as output_handle:\n",
    "        SeqIO.write(MSA_SeqRecords, output_handle, export_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e762fd19-93a8-480a-91c5-e1ea146a179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSA_filename = \"./data/protein-families-msa-full/PF00004_full.fasta\"\n",
    "all_seqs = [(record.description, remove_insertions(str(record.seq))) for record in SeqIO.parse(MSA_filename, \"fasta\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdfcc2de-b9ed-4996-aa66-f7d1512c5c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_set(seqs_per_MSA, n_sampled_MSAs, p_mask, mask_idx, seed):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    masked_MSAs = []\n",
    "    true_MSAs = []\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    for i in range(n_sampled_MSAs):\n",
    "    \n",
    "        sampled_ids = list(np.random.choice(range(len(all_seqs)), seqs_per_MSA))\n",
    "        sampled_MSA = [all_seqs[i] for i in sampled_ids]\n",
    "    \n",
    "        _,_,batch_tokens = batch_converter([sampled_MSA])\n",
    "    \n",
    "        starting_tokens = batch_tokens[0,:,:1]\n",
    "        batch_tokens = batch_tokens[0,:,1:]\n",
    "    \n",
    "        mask = ((torch.rand(batch_tokens.shape) > p_mask).type(\n",
    "                    torch.uint8))\n",
    "    \n",
    "        masked_batch_tokens = batch_tokens * mask + mask_idx * (1 - mask)\n",
    "    \n",
    "        batch_tokens = torch.cat((starting_tokens, batch_tokens), dim = -1)\n",
    "        masked_batch_tokens = torch.cat((starting_tokens, masked_batch_tokens), dim = -1)\n",
    "    \n",
    "        masked_MSAs.append(masked_batch_tokens)\n",
    "        true_MSAs.append(batch_tokens)\n",
    "    \n",
    "    masked_MSAs = torch.stack(masked_MSAs, dim=0).to(device)\n",
    "    true_MSAs = torch.stack(true_MSAs, dim=0).to(device)\n",
    "    \n",
    "    return masked_MSAs, true_MSAs\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5529404-042b-454b-959a-bf3285eff424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSADataset(Dataset):\n",
    "    def __init__(self, masked_tokens, true_tokens):\n",
    "        self.masked_tokens = masked_tokens\n",
    "        self.true_tokens = true_tokens\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.masked_tokens.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        masked_MSA, true_MSA = self.masked_tokens[idx], self.true_tokens[idx]\n",
    "\n",
    "        return masked_MSA, true_MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5773e8f4-cd67-4721-a5e4-ac7136fc4249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1/1000], Loss: 1.0809\n",
      "Epoch [1/5], Step [2/1000], Loss: 1.0954\n",
      "Epoch [1/5], Step [3/1000], Loss: 1.0653\n",
      "Epoch [1/5], Step [4/1000], Loss: 1.0506\n",
      "Epoch [1/5], Step [5/1000], Loss: 1.0405\n",
      "Epoch [1/5], Step [6/1000], Loss: 1.0230\n",
      "Epoch [1/5], Step [7/1000], Loss: 0.9821\n",
      "Epoch [1/5], Step [8/1000], Loss: 1.0436\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Step [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_total_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m masked_MSAs, true_MSAs, logits\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "num_epochs = 5\n",
    "batch_size = 1\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "# lr_scheduler: str = \"warmup_linear\"\n",
    "# warmup_steps: int = 16000\n",
    "adam_betas = (0.9, 0.999)\n",
    "max_steps: int = 1000000\n",
    "\n",
    "seqs_per_MSA = 100\n",
    "n_sampled_MSAs = 1000\n",
    "p_mask = 0.1\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "model = model.to(device)\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "mask_idx = alphabet.tok_to_idx[\"<mask>\"]\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if not name.startswith(\"lm_head\"):\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.lm_head.weight.requires_grad = True\n",
    "\n",
    "masked_MSAs, true_MSAs = create_training_set(seqs_per_MSA=seqs_per_MSA, n_sampled_MSAs= n_sampled_MSAs, p_mask=p_mask, mask_idx=mask_idx, seed=42)\n",
    "\n",
    "MSAs_Dataset = MSADataset(masked_MSAs,true_MSAs)\n",
    "train_dataloader = DataLoader(MSAs_Dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=adam_betas)  \n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(train_dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (masked_MSAs, true_MSAs) in enumerate(train_dataloader):  \n",
    "\n",
    "        logits = model(masked_MSAs)[\"logits\"]\n",
    "        masked_pos = masked_MSAs == alphabet.tok_to_idx[\"<mask>\"]\n",
    "\n",
    "        logits = logits[masked_pos].to(device)\n",
    "        true_MSAs = true_MSAs[masked_pos].to(device)\n",
    "\n",
    "        loss = criterion(logits, true_MSAs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 1 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        del masked_MSAs, true_MSAs, logits\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     n_correct = 0\n",
    "#     n_samples = 0\n",
    "#     for images, labels in test_loader:\n",
    "#         images = images.reshape(-1, 28*28).to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         # max returns (value ,index)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         n_samples += labels.size(0)\n",
    "#         n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     acc = 100.0 * n_correct / n_samples\n",
    "#     print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a205e8f-e157-410f-a990-224ad6b3be52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
